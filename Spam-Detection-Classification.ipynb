{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37c57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e87f90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"SMSSpamCollection.txt\", sep=\"\\t\", header=None,  names=[\"label\", \"Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc618ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43bfea10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd61d37d",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING \n",
    "* data cleaning\n",
    "* eliminate stop words\n",
    "* reduce size of the document using stemming or lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2741e",
   "metadata": {},
   "source": [
    "**CLEANING**\n",
    "* Removing punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43958707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string \n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22868e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Punctuation(text):\n",
    "    result = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2b81e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_clean'] = df['Content'].apply(lambda x : Remove_Punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44ad982f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                         Content_clean  \n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...  \n",
       "1                                                                              Ok lar Joking wif u oni  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...  \n",
       "3                                                          U dun say so early hor U c already then say  \n",
       "4                                          Nah I dont think he goes to usf he lives around here though  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc3dc4",
   "metadata": {},
   "source": [
    "* Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdbe6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def Tokenizer(text):\n",
    "    words = re.split('\\W+', text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0380b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_tokenized'] = df['Content_clean'].apply(lambda x : Tokenizer(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbe474ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_clean</th>\n",
       "      <th>Content_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                         Content_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                     Content_tokenized  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...  \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]  \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83dc6822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f92e027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopword = nltk.corpus.stopwords.words('english')\n",
    "en_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60359f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_Stopwords(content_list):\n",
    "    result = [word for word in content_list if word not in en_stopword]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a4c61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_without_Stopwords'] = df['Content_tokenized'].apply(lambda x: Remove_Stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f4ee66e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_clean</th>\n",
       "      <th>Content_tokenized</th>\n",
       "      <th>Content_without_Stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                         Content_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                     Content_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                             Content_without_Stopwords  \n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d1a49",
   "metadata": {},
   "source": [
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6389aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e508938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(words):\n",
    "    result = [ps.stem(word) for word in words]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11767f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Stemmed'] = df['Content_without_Stopwords'].apply(lambda x : stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5051e50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_clean</th>\n",
       "      <th>Content_tokenized</th>\n",
       "      <th>Content_without_Stopwords</th>\n",
       "      <th>Content_Stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                         Content_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                     Content_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                             Content_without_Stopwords  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                                                                       Content_Stemmed  \n",
       "0        [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]  \n",
       "1                                                                         [ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...  \n",
       "3                                                        [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "4                                                   [nah, dont, think, goe, usf, live, around, though]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd702bf",
   "metadata": {},
   "source": [
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5166dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82abc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(words):\n",
    "    result = [wn.lemmatize(word) for word in words]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bcb1c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d99dd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Lemmatized'] = df['Content_without_Stopwords'].apply(lambda x : lemmatizing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e4ee8e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_clean</th>\n",
       "      <th>Content_tokenized</th>\n",
       "      <th>Content_without_Stopwords</th>\n",
       "      <th>Content_Stemmed</th>\n",
       "      <th>Content_Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]</td>\n",
       "      <td>[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, then, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>Nah I dont think he goes to usf he lives around here though</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, though]</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, though]</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                         Content_clean  \\\n",
       "0  Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amo...   \n",
       "1                                                                              Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005 Text FA to 87121 to receive e...   \n",
       "3                                                          U dun say so early hor U c already then say   \n",
       "4                                          Nah I dont think he goes to usf he lives around here though   \n",
       "\n",
       "                                                                                     Content_tokenized  \\\n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, ci...   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, to...   \n",
       "3                                              [u, dun, say, so, early, hor, u, c, already, then, say]   \n",
       "4                            [nah, i, dont, think, he, goes, to, usf, he, lives, around, here, though]   \n",
       "\n",
       "                                                                             Content_without_Stopwords  \\\n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]   \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...   \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]   \n",
       "4                                                 [nah, dont, think, goes, usf, lives, around, though]   \n",
       "\n",
       "                                                                                       Content_Stemmed  \\\n",
       "0        [go, jurong, point, crazi, avail, bugi, n, great, world, la, e, buffet, cine, got, amor, wat]   \n",
       "1                                                                         [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, final, tkt, 21st, may, 2005, text, fa, 87121, receiv,...   \n",
       "3                                                        [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4                                                   [nah, dont, think, goe, usf, live, around, though]   \n",
       "\n",
       "                                                                                    Content_Lemmatized  \n",
       "0  [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]  \n",
       "1                                                                       [ok, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...  \n",
       "3                                                        [u, dun, say, early, hor, u, c, already, say]  \n",
       "4                                                    [nah, dont, think, go, usf, life, around, though]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "167af5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Content_tokenized','Content_without_Stopwords','Content_clean'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea05b95",
   "metadata": {},
   "source": [
    "## Vectorisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6946674a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2bc03",
   "metadata": {},
   "source": [
    "### Methode : Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22bdfde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64496bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(email):\n",
    "    result = \"\".join([word for word in email if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', result)\n",
    "    final_result = [ps.stem(word) for word in tokens if word not in en_stopword]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32bdd71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorisation_full = CountVectorizer(analyzer=clean_email)\n",
    "vect_final = vectorisation_full.fit_transform(df['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1129569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 8193)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1730203",
   "metadata": {},
   "source": [
    "### Methode :  N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70288a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aef5f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(email):\n",
    "    result = \"\".join([word for word in email if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', result)\n",
    "    final_result = \" \".join([ps.stem(word) for word in tokens if word not in en_stopword])\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0da2ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Stemed'] = df['Content'].apply(lambda x : clean_email(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6528417e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_Stemed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world la e buffet cine got amor wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                        Content_Stemed  \n",
       "0                         go jurong point crazi avail bugi n great world la e buffet cine got amor wat  \n",
       "1                                                                                ok lar joke wif u oni  \n",
       "2  free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...  \n",
       "3                                                                  u dun say earli hor u c alreadi say  \n",
       "4                                                          nah i dont think goe usf live around though  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d4f8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vectorisation = CountVectorizer(ngram_range=(2,2))\n",
    "vecteur_final = ngram_vectorisation.fit_transform(df['Content_Stemed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b710ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 34162)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecteur_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e41c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = df[0:5]\n",
    "ngram_vectorisation0 = CountVectorizer(ngram_range=(2,2))\n",
    "vecteur_final0 = ngram_vectorisation0.fit_transform(data0['Content_Stemed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e5df162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 50)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecteur_final0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c223312b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2005 text',\n",
       " '21st may',\n",
       " '87121 receiv',\n",
       " 'alreadi say',\n",
       " 'amor wat',\n",
       " 'appli 08452810075over18',\n",
       " 'around though',\n",
       " 'avail bugi',\n",
       " 'buffet cine',\n",
       " 'bugi great',\n",
       " 'cine got',\n",
       " 'comp win',\n",
       " 'crazi avail',\n",
       " 'cup final',\n",
       " 'dont think',\n",
       " 'dun say',\n",
       " 'earli hor',\n",
       " 'entri questionstd',\n",
       " 'entri wkli',\n",
       " 'fa 87121',\n",
       " 'fa cup',\n",
       " 'final tkt',\n",
       " 'free entri',\n",
       " 'go jurong',\n",
       " 'goe usf',\n",
       " 'got amor',\n",
       " 'great world',\n",
       " 'hor alreadi',\n",
       " 'joke wif',\n",
       " 'jurong point',\n",
       " 'la buffet',\n",
       " 'lar joke',\n",
       " 'live around',\n",
       " 'may 2005',\n",
       " 'nah dont',\n",
       " 'ok lar',\n",
       " 'point crazi',\n",
       " 'questionstd txt',\n",
       " 'ratetc appli',\n",
       " 'receiv entri',\n",
       " 'say earli',\n",
       " 'text fa',\n",
       " 'think goe',\n",
       " 'tkt 21st',\n",
       " 'txt ratetc',\n",
       " 'usf live',\n",
       " 'wif oni',\n",
       " 'win fa',\n",
       " 'wkli comp',\n",
       " 'world la']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorisation0.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "148c1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vect = pd.DataFrame(vecteur_final0.toarray())\n",
    "df_vect.columns = ngram_vectorisation0.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b9648ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2005 text</th>\n",
       "      <th>21st may</th>\n",
       "      <th>87121 receiv</th>\n",
       "      <th>alreadi say</th>\n",
       "      <th>amor wat</th>\n",
       "      <th>appli 08452810075over18</th>\n",
       "      <th>around though</th>\n",
       "      <th>avail bugi</th>\n",
       "      <th>buffet cine</th>\n",
       "      <th>bugi great</th>\n",
       "      <th>...</th>\n",
       "      <th>say earli</th>\n",
       "      <th>text fa</th>\n",
       "      <th>think goe</th>\n",
       "      <th>tkt 21st</th>\n",
       "      <th>txt ratetc</th>\n",
       "      <th>usf live</th>\n",
       "      <th>wif oni</th>\n",
       "      <th>win fa</th>\n",
       "      <th>wkli comp</th>\n",
       "      <th>world la</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2005 text  21st may  87121 receiv  alreadi say  amor wat  \\\n",
       "0          0         0             0            0         1   \n",
       "1          0         0             0            0         0   \n",
       "2          1         1             1            0         0   \n",
       "3          0         0             0            1         0   \n",
       "4          0         0             0            0         0   \n",
       "\n",
       "   appli 08452810075over18  around though  avail bugi  buffet cine  \\\n",
       "0                        0              0           1            1   \n",
       "1                        0              0           0            0   \n",
       "2                        1              0           0            0   \n",
       "3                        0              0           0            0   \n",
       "4                        0              1           0            0   \n",
       "\n",
       "   bugi great  ...  say earli  text fa  think goe  tkt 21st  txt ratetc  \\\n",
       "0           1  ...          0        0          0         0           0   \n",
       "1           0  ...          0        0          0         0           0   \n",
       "2           0  ...          0        1          0         1           1   \n",
       "3           0  ...          1        0          0         0           0   \n",
       "4           0  ...          0        0          1         0           0   \n",
       "\n",
       "   usf live  wif oni  win fa  wkli comp  world la  \n",
       "0         0        0       0          0         1  \n",
       "1         0        1       0          0         0  \n",
       "2         0        0       1          1         0  \n",
       "3         0        0       0          0         0  \n",
       "4         1        0       0          0         0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8b062",
   "metadata": {},
   "source": [
    "## Vectoriation TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44481267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_Stemed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world la e buffet cine got amor wat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah i dont think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u. U have won the 750 Pound prize. 2 claim is easy...</td>\n",
       "      <td>thi 2nd time tri 2 contact u u 750 pound prize 2 claim easi call 087187272008 now1 onli 10p per ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will  b going to esplanade fr home?</td>\n",
       "      <td>will  b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other suggestions?</td>\n",
       "      <td>piti mood soani suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd be interested in buying something else next week ...</td>\n",
       "      <td>the guy bitch i act like id interest buy someth els next week gave us free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl it true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  \\\n",
       "0      ham   \n",
       "1      ham   \n",
       "2     spam   \n",
       "3      ham   \n",
       "4      ham   \n",
       "...    ...   \n",
       "5567  spam   \n",
       "5568   ham   \n",
       "5569   ham   \n",
       "5570   ham   \n",
       "5571   ham   \n",
       "\n",
       "                                                                                                  Content  \\\n",
       "0     Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                           Ok lar... Joking wif u oni...   \n",
       "2     Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                       U dun say so early hor... U c already then say...   \n",
       "4                                           Nah I don't think he goes to usf, he lives around here though   \n",
       "...                                                                                                   ...   \n",
       "5567  This is the 2nd time we have tried 2 contact u. U have won the 750 Pound prize. 2 claim is easy...   \n",
       "5568                                                                 Will  b going to esplanade fr home?   \n",
       "5569                                            Pity, * was in mood for that. So...any other suggestions?   \n",
       "5570  The guy did some bitching but I acted like i'd be interested in buying something else next week ...   \n",
       "5571                                                                           Rofl. Its true to its name   \n",
       "\n",
       "                                                                                           Content_Stemed  \n",
       "0                            go jurong point crazi avail bugi n great world la e buffet cine got amor wat  \n",
       "1                                                                                   ok lar joke wif u oni  \n",
       "2     free entri 2 wkli comp win fa cup final tkt 21st may 2005 text fa 87121 receiv entri questionstd...  \n",
       "3                                                                     u dun say earli hor u c alreadi say  \n",
       "4                                                             nah i dont think goe usf live around though  \n",
       "...                                                                                                   ...  \n",
       "5567  thi 2nd time tri 2 contact u u 750 pound prize 2 claim easi call 087187272008 now1 onli 10p per ...  \n",
       "5568                                                                         will  b go esplanad fr home  \n",
       "5569                                                                              piti mood soani suggest  \n",
       "5570                           the guy bitch i act like id interest buy someth els next week gave us free  \n",
       "5571                                                                                    rofl it true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e9dcc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_email(email):\n",
    "    result = \"\".join([word for word in email if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', result)\n",
    "    final_result = [ps.stem(word) for word in tokens if word not in en_stopword]\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f99311ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ea9b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorisation = TfidfVectorizer(analyzer=clean_email)\n",
    "tfidf_final = tfidf_vectorisation.fit_transform(df['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3762651e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 8193)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "809caa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = df[0:5]\n",
    "tfidf_vectorisation0 = TfidfVectorizer(analyzer=clean_email)\n",
    "tfidf_final0 = tfidf_vectorisation0.fit_transform(data0['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33ae007a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 58)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_final0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd9e997f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['08452810075over18',\n",
       " '2',\n",
       " '2005',\n",
       " '21st',\n",
       " '87121',\n",
       " 'alreadi',\n",
       " 'amor',\n",
       " 'appli',\n",
       " 'around',\n",
       " 'avail',\n",
       " 'buffet',\n",
       " 'bugi',\n",
       " 'c',\n",
       " 'cine',\n",
       " 'comp',\n",
       " 'crazi',\n",
       " 'cup',\n",
       " 'dont',\n",
       " 'dun',\n",
       " 'e',\n",
       " 'earli',\n",
       " 'entri',\n",
       " 'fa',\n",
       " 'final',\n",
       " 'free',\n",
       " 'go',\n",
       " 'goe',\n",
       " 'got',\n",
       " 'great',\n",
       " 'hor',\n",
       " 'i',\n",
       " 'joke',\n",
       " 'jurong',\n",
       " 'la',\n",
       " 'lar',\n",
       " 'live',\n",
       " 'may',\n",
       " 'n',\n",
       " 'nah',\n",
       " 'ok',\n",
       " 'oni',\n",
       " 'point',\n",
       " 'questionstd',\n",
       " 'ratetc',\n",
       " 'receiv',\n",
       " 'say',\n",
       " 'text',\n",
       " 'think',\n",
       " 'though',\n",
       " 'tkt',\n",
       " 'txt',\n",
       " 'u',\n",
       " 'usf',\n",
       " 'wat',\n",
       " 'wif',\n",
       " 'win',\n",
       " 'wkli',\n",
       " 'world']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorisation0.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d81139ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tdidf = pd.DataFrame(tfidf_final0.toarray())\n",
    "df_tdidf.columns = tfidf_vectorisation0.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df912db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>08452810075over18</th>\n",
       "      <th>2</th>\n",
       "      <th>2005</th>\n",
       "      <th>21st</th>\n",
       "      <th>87121</th>\n",
       "      <th>alreadi</th>\n",
       "      <th>amor</th>\n",
       "      <th>appli</th>\n",
       "      <th>around</th>\n",
       "      <th>avail</th>\n",
       "      <th>...</th>\n",
       "      <th>though</th>\n",
       "      <th>tkt</th>\n",
       "      <th>txt</th>\n",
       "      <th>u</th>\n",
       "      <th>usf</th>\n",
       "      <th>wat</th>\n",
       "      <th>wif</th>\n",
       "      <th>win</th>\n",
       "      <th>wkli</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.339393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.420669</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.19245</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.293564</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.473691</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   08452810075over18        2     2005     21st    87121   alreadi  amor  \\\n",
       "0            0.00000  0.00000  0.00000  0.00000  0.00000  0.000000  0.25   \n",
       "1            0.00000  0.00000  0.00000  0.00000  0.00000  0.000000  0.00   \n",
       "2            0.19245  0.19245  0.19245  0.19245  0.19245  0.000000  0.00   \n",
       "3            0.00000  0.00000  0.00000  0.00000  0.00000  0.293564  0.00   \n",
       "4            0.00000  0.00000  0.00000  0.00000  0.00000  0.000000  0.00   \n",
       "\n",
       "     appli    around  avail  ...    though      tkt      txt         u  \\\n",
       "0  0.00000  0.000000   0.25  ...  0.000000  0.00000  0.00000  0.000000   \n",
       "1  0.00000  0.000000   0.00  ...  0.000000  0.00000  0.00000  0.339393   \n",
       "2  0.19245  0.000000   0.00  ...  0.000000  0.19245  0.19245  0.000000   \n",
       "3  0.00000  0.000000   0.00  ...  0.000000  0.00000  0.00000  0.473691   \n",
       "4  0.00000  0.333333   0.00  ...  0.333333  0.00000  0.00000  0.000000   \n",
       "\n",
       "        usf   wat       wif      win     wkli  world  \n",
       "0  0.000000  0.25  0.000000  0.00000  0.00000   0.25  \n",
       "1  0.000000  0.00  0.420669  0.00000  0.00000   0.00  \n",
       "2  0.000000  0.00  0.000000  0.19245  0.19245   0.00  \n",
       "3  0.000000  0.00  0.000000  0.00000  0.00000   0.00  \n",
       "4  0.333333  0.00  0.000000  0.00000  0.00000   0.00  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tdidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb4992",
   "metadata": {},
   "source": [
    "**Feature engineering**\n",
    "* Adding new variables\n",
    "* transform the existant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a096bf",
   "metadata": {},
   "source": [
    "## Adding new variables for the SPAM MODEL\n",
    "* rate of punctuation marks\n",
    "* Message size\n",
    "* rate uppercase letters\n",
    "* in spam we will se never words like 'lol' so we can create boolean if lol is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea91e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee272c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "   Content_len  \n",
       "0           92  \n",
       "1           24  \n",
       "2          128  \n",
       "3           39  \n",
       "4           49  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Content_len'] = data['Content'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ef99529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "def count_punctuation(text):\n",
    "    binary_array = [1 for ch in text if ch in string.punctuation]\n",
    "    nb_punctuation = sum(binary_array)\n",
    "    total = len(text) - text.count(\" \")\n",
    "    return round(nb_punctuation/(total), 4)*100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeefebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['punctuation_rate'] = data['Content'].apply(lambda x : count_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7fcb204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_len</th>\n",
       "      <th>punctuation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>92</td>\n",
       "      <td>9.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>24</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>39</td>\n",
       "      <td>15.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                               Content  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "   Content_len  punctuation_rate  \n",
       "0           92              9.78  \n",
       "1           24             25.00  \n",
       "2          128              4.69  \n",
       "3           39             15.38  \n",
       "4           49              4.08  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc7fc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2001b678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(data[data['label']=='spam']['Content_len'], bins, alpha=0.5, density=True, stacked=True, label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['Content_len'], bins, alpha=0.5, density=True, stacked=True, label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c99fda4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(data[data['label']=='spam']['punctuation_rate'], bins, alpha=0.5, density=True, stacked=True, label='spam')\n",
    "pyplot.hist(data[data['label']=='ham']['punctuation_rate'], bins, alpha=0.5, density=True, stacked=True, label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09994e",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f2c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
